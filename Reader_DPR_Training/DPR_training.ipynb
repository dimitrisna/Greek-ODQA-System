{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0384ifx5szPa",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training Your Own \"Dense Passage Retrieval\" Model\n",
        "\n",
        "\n",
        "Haystack contains all the tools needed to train your own Dense Passage Retrieval model.\n",
        "This tutorial will guide you through the steps required to create a retriever that is specifically tailored to your domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T09:20:53.128642Z",
          "iopub.status.busy": "2022-06-27T09:20:53.128327Z",
          "iopub.status.idle": "2022-06-27T09:22:03.464834Z",
          "shell.execute_reply": "2022-06-27T09:22:03.463592Z",
          "shell.execute_reply.started": "2022-06-27T09:20:53.128568Z"
        },
        "id": "b9_IVcSBszPd",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T09:22:03.467055Z",
          "iopub.status.busy": "2022-06-27T09:22:03.466842Z",
          "iopub.status.idle": "2022-06-27T09:22:09.119325Z",
          "shell.execute_reply": "2022-06-27T09:22:09.118580Z",
          "shell.execute_reply.started": "2022-06-27T09:22:03.467030Z"
        },
        "id": "0Q8sCkW3szPe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Here are some imports that we'll need\n",
        "\n",
        "from haystack.nodes import DensePassageRetriever\n",
        "from haystack.utils import fetch_archive_from_http\n",
        "from haystack.document_stores import InMemoryDocumentStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S05lhy5bszPf",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training Data\n",
        "\n",
        "DPR training performed using Information Retrieval data.\n",
        "More specifically, you want to feed in pairs of queries and relevant documents.\n",
        "\n",
        "To train a model, we will need a dataset that has the same format as the original DPR training data.\n",
        "Each data point in the dataset should have the following dictionary structure.\n",
        "\n",
        "``` python\n",
        "    {\n",
        "        \"dataset\": str,\n",
        "        \"question\": str,\n",
        "        \"answers\": list of str\n",
        "        \"positive_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "        \"negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "        \"hard_negative_ctxs\": list of dictionaries of format {'title': str, 'text': str, 'score': int, 'title_score': int, 'passage_id': str}\n",
        "    }\n",
        "```\n",
        "\n",
        "`positive_ctxs` are context passages which are relevant to the query.\n",
        "In some datasets, queries might have more than one positive context\n",
        "in which case you can set the `num_positives` parameter to be higher than the default 1.\n",
        "Note that `num_positives` needs to be lower or equal to the minimum number of `positive_ctxs` for queries in your data.\n",
        "If you have an unequal number of positive contexts per example,\n",
        "you might want to generate some soft labels by retrieving similar contexts which contain the answer.\n",
        "\n",
        "DPR is standardly trained using a method known as in-batch negatives.\n",
        "This means that positive contexts for a given query are treated as negative contexts for the other queries in the batch.\n",
        "Doing so allows for a high degree of computational efficiency, thus allowing the model to be trained on large amounts of data.\n",
        "\n",
        "`negative_ctxs` is not actually used in Haystack's DPR training so we recommend you set it to an empty list.\n",
        "They were used by the original DPR authors in an experiment to compare it against the in-batch negatives method.\n",
        "\n",
        "`hard_negative_ctxs` are passages that are not relevant to the query.\n",
        "In the original DPR paper, these are fetched using a retriever to find the most relevant passages to the query.\n",
        "Passages which contain the answer text are filtered out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pas1QqYbszPg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Download Data\n",
        "\n",
        "Download the necessary data for dpr training\n",
        "The greek translation of the original NQ dataset for dpr training can be downloaded by executing the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1UA7_ouoR8WDeeNReYrZZS_PIXvGiP7Md\n",
        "!gdown https://drive.google.com/uc?id=1m52D8QXIxxr5914ApldX3M1A547Z_mGD"
      ],
      "metadata": {
        "id": "rdEUb2vdhDfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DA1yPtWszPh",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training DPR from Scratch\n",
        "\n",
        "The default variables that we provide below are chosen to train a DPR model from scratch.\n",
        "Here, both passage and query embedding models are initialized using BERT base\n",
        "and the model is trained using Google's Natural Questions dataset (in a format specialised for DPR).\n",
        "\n",
        "If you are working in a language other than English,\n",
        "you will want to initialize the passage and query embedding models with a language model that supports your language\n",
        "and also provide a dataset in your language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T09:22:09.121580Z",
          "iopub.status.busy": "2022-06-27T09:22:09.120994Z",
          "iopub.status.idle": "2022-06-27T09:22:09.126821Z",
          "shell.execute_reply": "2022-06-27T09:22:09.125829Z",
          "shell.execute_reply.started": "2022-06-27T09:22:09.121548Z"
        },
        "id": "Eoh7u8HoszPi",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Here are the variables to specify our training data, the models that we use to initialize DPR\n",
        "# and the directory where we'll be saving the model\n",
        "\n",
        "train_filename = \"nq_train_dpr.json\"\n",
        "dev_filename = \"nq_dev_dpr.json\"\n",
        "\n",
        "query_model = \"nlpaueb/bert-base-greek-uncased-v1\"\n",
        "passage_model = \"nlpaueb/bert-base-greek-uncased-v1\"\n",
        "\n",
        "save_dir = \"./dpr\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upfJ1fZHszPj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Initialization\n",
        "\n",
        "Here we want to initialize our model either with plain language model weights for training from scratch\n",
        "or else with pretrained DPR weights for finetuning.\n",
        "We follow the [original DPR parameters](https://github.com/facebookresearch/DPR#best-hyperparameter-settings)\n",
        "for their max passage length but set max query length to 64 since queries are very rarely longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T09:22:09.129962Z",
          "iopub.status.busy": "2022-06-27T09:22:09.128709Z",
          "iopub.status.idle": "2022-06-27T09:22:11.295883Z",
          "shell.execute_reply": "2022-06-27T09:22:11.294850Z",
          "shell.execute_reply.started": "2022-06-27T09:22:09.129873Z"
        },
        "id": "47RQKir7szPk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "## Initialize DPR model\n",
        "\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=InMemoryDocumentStore(),\n",
        "    query_embedding_model=query_model,\n",
        "    passage_embedding_model=passage_model,\n",
        "    max_seq_len_query=64,\n",
        "    max_seq_len_passage=256,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7nirAzlszPk",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's start training and save our trained model!\n",
        "\n",
        "On a V100 GPU, you can fit up to batch size 16 so we set gradient accumulation steps to 8 in order\n",
        "to simulate the batch size 128 of the original DPR experiment.\n",
        "\n",
        "When `embed_title=True`, the document title is prepended to the input text sequence with a `[SEP]` token\n",
        "between it and document text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T09:22:27.589740Z",
          "iopub.status.busy": "2022-06-27T09:22:27.589462Z",
          "iopub.status.idle": "2022-06-27T09:22:27.609119Z",
          "shell.execute_reply": "2022-06-27T09:22:27.607887Z",
          "shell.execute_reply.started": "2022-06-27T09:22:27.589715Z"
        },
        "id": "kgLKKOR6szPk",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Start training our model and save it when it is finished\n",
        "\n",
        "retriever.train(\n",
        "    data_dir=\".\",\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=40,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=5000,\n",
        "    embed_title=False,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        "    checkpoint_root_dir = \"./model_checkpoints\",\n",
        "    checkpoint_every =5000,\n",
        "    checkpoints_to_keep= 2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR4s5cBuszPl",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Loading\n",
        "\n",
        "Loading our newly trained model is simple!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyrGLrBWszPl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "reloaded_retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll6K_TqQszPl"
      },
      "source": [
        "\n",
        "\n",
        "This notebook is based on the [Haystack](https://github.com/deepset-ai/haystack/) notebook for DPR training."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DPR_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}